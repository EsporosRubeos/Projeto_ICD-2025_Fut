{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import requests\n",
    "from random import uniform\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming convention for the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_12 = [\n",
    "    'serie_A_overall',\n",
    "    'serie_A_homeaway',\n",
    "    'squad_standard_stats',\n",
    "    'squad_standard_stats_opp',\n",
    "    'squad_goalkeeping',\n",
    "    'squad_goalkeeping_opp',\n",
    "    'squad_advanced_goalkeeping',\n",
    "    'squad_advanced_goalkeeping_opp',\n",
    "    'squad_shooting',\n",
    "    'squad_shooting_opp',\n",
    "    'squad_passing',\n",
    "    'squad_passing_opp',\n",
    "    'squad_pass_types',\n",
    "    'squad_pass_types_opp',\n",
    "    'squad_goal_shot_creation',\n",
    "    'squad_goal_shot_creation_opp',\n",
    "    'squad_defensive_actions',\n",
    "    'squad_defensive_actions_opp',\n",
    "    'squad_possession',\n",
    "    'squad_possession_opp',\n",
    "    'squad_playing_time',\n",
    "    'squad_playing_time_opp',\n",
    "    'squad_miscellaneous',\n",
    "    'squad_miscellaneous_opp',\n",
    "]\n",
    "\n",
    "titles_6 = titles_12[:6] + titles_12[8:10] + titles_12[20:]\n",
    "\n",
    "titles_5 = titles_6[:6] + titles_6[8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire the league data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/clubes/', exist_ok=True)\n",
    "os.makedirs('data/league/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all tables from the webpage\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for page in range(11):\n",
    "    \n",
    "    url = f\"https://fbref.com/en/comps/24/{2024 - page}/{2024 - page}-Serie-A-Stats\"\n",
    "    tables = pd.read_html(url)\n",
    "    year = 2024 - page\n",
    "    \n",
    "    # Convert each table to a DataFrame\n",
    "    if year > 2018:\n",
    "        dfs = {f\"{titles_12[i]}_{year}\": table for i, table in enumerate(tables)}\n",
    "    \n",
    "    elif year > 2015:\n",
    "        dfs = {f\"{titles_6[i]}_{year}\": table for i, table in enumerate(tables)}\n",
    "    \n",
    "    else:\n",
    "        dfs = {f\"{titles_5[i]}_{year}\": table for i, table in enumerate(tables)}\n",
    "    \n",
    "    for name, df in dfs.items():\n",
    "        df.to_csv(f\"data/league/{name}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire player data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ID da página de acesso de cada clube\n",
    "clubes = {\n",
    "        'Botafogo-RJ': {'id': 'd9fdd9d9'},\n",
    "        'Palmeiras': {'id': 'abdce579'},\n",
    "        'Flamengo': {'id': '639950ae'},\n",
    "        'Fortaleza': {'id': 'a9d0ab0e'},\n",
    "        'Internacional': {'id': '6f7e1f03'},\n",
    "        'São-Paulo': {'id': '5f232eb1'},\n",
    "        'Corinthians': {'id': 'bf4acd28'},\n",
    "        'Bahia': {'id': '157b7fee'},\n",
    "        'Cruzeiro': {'id': '03ff5eeb'},\n",
    "        'Vasco-da-Gama': {'id': '83f55dbe'},\n",
    "        'Vitória': {'id': '33f95fe0'},\n",
    "        'Atlético-Mineiro': {'id': '422bb734'},\n",
    "        'Fluminense': {'id': '84d9701c'},\n",
    "        'Grêmio': {'id': 'd5ae3703'},\n",
    "        'Juventude': {'id': 'd081b697'},\n",
    "        'RB-Bragantino': {'id': 'f98930d1'},\n",
    "        'Ath-Paranaense': {'id': '2091c619'},\n",
    "        'Criciúma': {'id': '3f7595bb'},\n",
    "        'Atl-Goianiense': {'id': '32d508ca'},\n",
    "        'Cuiabá': {'id': 'f0e6fb14'},\n",
    "        'Santos': {'id': '712c528f'},\n",
    "        'Goiás': {'id': '78c617cc'},\n",
    "        'Coritiba': {'id': 'd680d257'},\n",
    "        'América-MG': {'id': '1f68d780'},\n",
    "        'Ceará': {'id': '2f335e17'},\n",
    "        'Avaí': {'id': 'f205258a'},\n",
    "        'Sport-Recife': {'id': 'ece66b78'},\n",
    "        'Chapecoense': {'id': 'baa296ad'},\n",
    "        'CSA': {'id': '05aff519'},\n",
    "        'Paraná': {'id': '2091c619'},\n",
    "        'Ponte-Preta': {'id': 'b162ebe7'},\n",
    "        'Figueirense': {'id': '0ce4436d'},\n",
    "        'Santa-Cruz': {'id': 'ad0c1246'},\n",
    "        'Joinville': {'id': 'da0666a2'},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clube in clubes:\n",
    "    clubes[clube]['years_SA'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifica se o time estava na serie A naquele ano e adiciona o ano na key 'years_SA'\n",
    "\n",
    "for i in range(11):\n",
    "    df = pd.read_csv(f'data/league/serie_A_overall_{2024 - i}.csv')\n",
    "\n",
    "    unique_values = df['Squad'].unique()\n",
    "\n",
    "    for value in unique_values:\n",
    "        value = value.replace(\" \", \"-\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "        clubes[value]['years_SA'].append(2024 - i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping with pd.read_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clube, dados in clubes.items():\n",
    "    # Create club directory if it doesn't exist\n",
    "    club_dir = Path(f'data/clubes/{clube}')\n",
    "    club_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for year in dados['years_SA']:\n",
    "        # Generate expected filenames pattern for this year\n",
    "        existing_files = list(club_dir.glob(f\"{year}_*.csv\"))\n",
    "        \n",
    "        # Skip if we already have files for this year\n",
    "        if existing_files:\n",
    "            print(f\"Skipping {clube} {year} - already downloaded\")\n",
    "            continue\n",
    "        \n",
    "        url = f\"https://fbref.com/en/squads/{clubes[clube]['id']}/{year}/{clube}-Stats\"\n",
    "\n",
    "        try:\n",
    "            # Add delay to avoid 429 errors\n",
    "            time.sleep(uniform(3, 20))\n",
    "            tables = pd.read_html(url)\n",
    "            \n",
    "            # Save each table\n",
    "            for i, table in enumerate(tables):\n",
    "                filename = club_dir / f\"{year}_{i}.csv\"\n",
    "                table.to_csv(filename, index=False)\n",
    "                print(f\"Saved {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {clube} {year}: {str(e)}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://fbref.com/en/squads/d9fdd9d9/2022/Botafogo-RJ-Stats\"\n",
    "    \n",
    "\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Save each table\n",
    "for i, table in enumerate(tables):\n",
    "    filename = 'data/clubes/Botafogo-RJ' / f\"{year}_{i}.csv\"\n",
    "    table.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping with beautifulsoup (gerado pelo gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure scraping\n",
    " # Random delay between requests\n",
    "\n",
    "def scrape_table_without_readhtml(url):\n",
    "    \"\"\"Scrape HTML tables without pd.read_html()\"\"\"\n",
    "    ua = UserAgent()\n",
    "    headers = {'User-Agent': ua.random}\n",
    "    DELAY = (3, 7) \n",
    "    \n",
    "    try:\n",
    "        # Make request with delay\n",
    "        time.sleep(random.uniform(*DELAY))\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        tables = soup.find_all('table')\n",
    "        \n",
    "        dataframes = []\n",
    "        for table in tables:\n",
    "            # Extract headers\n",
    "            headers = []\n",
    "            thead = table.find('thead')\n",
    "            if thead:\n",
    "                for th in thead.find_all('th'):\n",
    "                    headers.append(th.get_text(strip=True))\n",
    "            \n",
    "            # Extract rows\n",
    "            rows = []\n",
    "            tbody = table.find('tbody') or table\n",
    "            for tr in tbody.find_all('tr'):\n",
    "                row = []\n",
    "                for td in tr.find_all(['td', 'th']):\n",
    "                    row.append(td.get_text(strip=True))\n",
    "                if row:\n",
    "                    rows.append(row)\n",
    "            \n",
    "            # Create DataFrame\n",
    "            if headers and rows:\n",
    "                df = pd.DataFrame(rows, columns=headers)\n",
    "                dataframes.append(df)\n",
    "        \n",
    "        return dataframes\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# # Example usage\n",
    "# url = \"https://fbref.com/en/squads/18bb7c10/2022/Arsenal-Stats\"\n",
    "# tables = scrape_table_without_readhtml(url)\n",
    "\n",
    "# for i, df in enumerate(tables):\n",
    "#     print(f\"Table {i+1}:\")\n",
    "#     print(df.head())\n",
    "#     df.to_csv(f\"table_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://fbref.com/en/squads/d9fdd9d9/2022/Botafogo-RJ-Stats: 403 Client Error: Forbidden for url: https://fbref.com/en/squads/d9fdd9d9/2022/Botafogo-RJ-Stats\n"
     ]
    }
   ],
   "source": [
    "url = f\"https://fbref.com/en/squads/d9fdd9d9/2022/Botafogo-RJ-Stats\"\n",
    "tables = scrape_table_without_readhtml(url)\n",
    "\n",
    "for i, df in enumerate(tables):\n",
    "    print(f\"Table {i+1}:\")\n",
    "    print(df.head())\n",
    "    df.to_csv(f\"table_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clube, dados in clubes.items():\n",
    "    # Create club directory if it doesn't exist\n",
    "    club_dir = Path(f'data/clubes/{clube}')\n",
    "    club_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for year in dados['years_SA']:\n",
    "        # Generate expected filenames pattern for this year\n",
    "        existing_files = list(club_dir.glob(f\"{year}_*.csv\"))\n",
    "        \n",
    "        # Skip if we already have files for this year\n",
    "        if existing_files:\n",
    "            print(f\"Skipping {clube} {year} - already downloaded\")\n",
    "            continue\n",
    "            \n",
    "        url = f\"https://fbref.com/en/squads/{clubes[clube]['id']}/{year}/{clube}-Stats\"\n",
    "\n",
    "        try:\n",
    "            tables = scrape_table_without_readhtml(url)\n",
    "            \n",
    "            for i, table in enumerate(tables):\n",
    "                filename = club_dir / f\"{year}_{i}.csv\"\n",
    "                table.to_csv(filename, index=False)\n",
    "                print(f\"Saved {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {clube} {year}: {str(e)}\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
